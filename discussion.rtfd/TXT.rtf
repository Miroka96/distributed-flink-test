{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww20000\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Exercise 4\
\
word count\
On one local machine:\
On could assume it would be disk bound, as the computation is not very intensive, however, the timeline (Fig 1) clearly shows that most time is spend aggregating the sum. \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic.png \width23320 \height7760 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 This might be because data has to be synchronised between the threads that filter the data and the threads that sum the word count. The window is keyed, meaning that a word that is processed by any thread will have to be send to a specific thread for sum aggregation. So the program is somewhat network (in this case because it is local inter thread communication) bound.\
\
k-means\
Local execution on one machine.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic 3.png \width23280 \height14040 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 As can be seen by the graphic the program is highly compute bound and spends most of its time in map and reduce. However the program is also network bound as it needs to synchronise the centroids after every iteration. Memory only comes in to play if larger files are analysed. Increasing the iteration only has effects on network and cpu. Increasing k has a small effect on memory as more centroids have to be stored, however, it is negligible compared to increasing the changing from berlin.csv to germany.csv. Using the germany file will also require a lot more computational effort. Disk is not a limiting factor as it only requires a fraction of the time, as can be seen in the picture. \
Looking at the dashboard, the data already seems to be well partitioned by flink. In all. tasks each instance runs with a similar amount of data, only in one case when computing the distinct lte towers, on instance does not produce any results.  \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
}